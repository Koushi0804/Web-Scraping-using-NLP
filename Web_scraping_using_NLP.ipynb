{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7pny9Rv20Vg",
        "outputId": "91f22b9a-3a0a-446e-a87a-2fbb04340937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import textstat\n",
        "from textstat import *\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='extraction_log.txt', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Download NLTK data if not already downloaded\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to perform textual analysis and compute variables\n",
        "def perform_text_analysis(text, stop_words, positive_words, negative_words):\n",
        "    # Clean text by removing stop words\n",
        "    cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sia.polarity_scores(cleaned_text)\n",
        "    positive_score = sentiment_scores['pos']\n",
        "    negative_score = -sentiment_scores['neg']  # Make it positive\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "\n",
        "    # Count positive and negative words\n",
        "    positive_words_count = len([word for word in cleaned_text.split() if word in positive_words])\n",
        "    negative_words_count = len([word for word in cleaned_text.split() if word in negative_words])\n",
        "\n",
        "    # Subjectivity Score\n",
        "    total_words_after_cleaning = len(cleaned_text.split())\n",
        "    subjectivity_score = (positive_words_count + negative_words_count) / (total_words_after_cleaning + 0.000001)\n",
        "\n",
        "    # Other variables\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "    words = word_tokenize(cleaned_text)\n",
        "    avg_sentence_length = round(len(words) / len(sentences), 2)\n",
        "    complex_word_count = textstat.difficult_words(cleaned_text)\n",
        "    word_count = len(words)\n",
        "    percentage_complex_words = (complex_word_count / word_count) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = round(word_count / len(sentences), 2)\n",
        "\n",
        "    # Count syllables per word\n",
        "    syllables_per_word = 0\n",
        "    for word in cleaned_text.split():\n",
        "        # Handle exceptions for syllable counting\n",
        "        word = re.sub(r'[?!,.]', '', word)  # Remove punctuation\n",
        "        if word.endswith(('es', 'ed')):\n",
        "            word = word[:-2]  # Remove common suffixes\n",
        "        syllables = textstat.syllable_count(word)\n",
        "        syllables_per_word += syllables\n",
        "\n",
        "    # Count personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', cleaned_text, flags=re.IGNORECASE))\n",
        "\n",
        "    # Average word length\n",
        "    avg_word_length = round(sum(len(word) for word in words) / len(words), 2)\n",
        "\n",
        "    return [positive_score, negative_score, polarity_score, avg_sentence_length,\n",
        "            percentage_complex_words, fog_index, avg_words_per_sentence,\n",
        "            complex_word_count, word_count, syllables_per_word, personal_pronouns,\n",
        "            avg_word_length]\n",
        "\n",
        "# Define lists to store data for both successful and error URLs\n",
        "data = []\n",
        "\n",
        "# Specify the folder containing stop words\n",
        "stopwords_folder = '/content/drive/MyDrive/StopWords'  # Replace with the path to your folder\n",
        "\n",
        "# List all the .txt files in the folder\n",
        "txt_files = [f for f in os.listdir(stopwords_folder) if f.endswith('.txt')]\n",
        "\n",
        "# Read the content of each .txt file\n",
        "stop_words = set()  # Create a set to store stop words\n",
        "\n",
        "for txt_file in txt_files:\n",
        "    file_path = os.path.join(stopwords_folder, txt_file)\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        file_content = file.read()\n",
        "        stop_words.update(file_content.split())\n",
        "\n",
        "# Read positive and negative words\n",
        "with open('/content/drive/MyDrive/Copy of positive-words.txt', 'r') as positive_words_file:\n",
        "    positive_words = positive_words_file.read().splitlines()\n",
        "\n",
        "import chardet\n",
        "\n",
        "# Detect file encoding\n",
        "with open('/content/drive/MyDrive/Copy of negative-words.txt', 'rb') as file:\n",
        "    result = chardet.detect(file.read())\n",
        "\n",
        "# Use detected encoding to open the file\n",
        "file_encoding = result['encoding']\n",
        "with open('/content/drive/MyDrive/Copy of negative-words.txt', 'r', encoding=file_encoding) as negative_words_file:\n",
        "    negative_words = negative_words_file.read().splitlines()\n",
        "\n",
        "# Load input data\n",
        "input_data = pd.read_excel('/content/drive/MyDrive/Copy of Input.xlsx')\n",
        "\n",
        "# Function to extract article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract title and article text, exclude unwanted content\n",
        "        title = soup.find('title').get_text()\n",
        "        article_text = soup.find('article').get_text()  # Adjust as per HTML structure\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        # Log the error and return None values\n",
        "        logging.error(f\"Error extracting text from URL: {url}\\nError message: {str(e)}\")\n",
        "        return None, None\n",
        "num_analysis_columns = 12  # Update this with the actual number of analysis columns\n",
        "# Iterate through URLs and save extracted data\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, article_text = extract_article_text(url)\n",
        "    if title and article_text:\n",
        "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write(f'Title: {title}\\n\\n')\n",
        "            file.write(article_text)\n",
        "        # Perform text analysis and store data in the appropriate list (success or error)\n",
        "        variables = perform_text_analysis(article_text, stop_words, positive_words, negative_words)\n",
        "        data.append([url_id, url, 'Success'] + variables)\n",
        "    else:\n",
        "        # Record the URL as an error with \"None\" values for analysis columns\n",
        "        data.append([url_id, url, 'Error'] + [None] * num_analysis_columns)\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=['URL_ID', 'URL','Status','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "                                 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "                                 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "                                 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD',\n",
        "                                 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel('/content/drive/MyDrive/Output.xlsx',index = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEJwGrSg5-yT",
        "outputId": "8a71d0a5-954e-4ff0-bd5d-a9fc44a9ceae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "ERROR:root:Error extracting text from URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Error message: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "ERROR:root:Error extracting text from URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Error message: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sv_5F2G7RMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}